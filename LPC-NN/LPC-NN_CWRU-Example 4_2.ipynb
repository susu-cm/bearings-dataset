{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CWRU-Case 2. Example for paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt;\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of trials that we want to do:\n",
    "number_Of_Trials=10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root folder of the datasets\n",
    "root_folder_=\"C:/Users/Olga/Desktop/CRWU/Datasets/\";\n",
    "root_folder_Models=\"d:/Data/Models1\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "myN=10;\n",
    "\n",
    "# Buidling neural network model\n",
    "def build_my_model(numberOfClasses):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=50, activation=\"tanh\"))\n",
    "    model.add(Dense(numberOfClasses))\n",
    "    #model.add(Dense(numberOfClasses, activation=\"softmax\"))\n",
    "    #model.compile(loss=\"categorical_crossentropy\",optimizer='adam', metrics=[\"accuracy\"]) \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-4, decay=1e-3 / 200)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training number_Of_Trials runs\n",
    "def training(myN, number_Of_Trials, model_name):\n",
    "    for i in range(number_Of_Trials):\n",
    "        print(\"Run N %d\" % (i+1))\n",
    "        #Loading training data\n",
    "        data = pd.read_csv(path_to_train,header=None)\n",
    "        data = data.iloc[np.random.permutation(len(data))]\n",
    "        labels=data.iloc[:,51]\n",
    "        features = data.iloc[:,1:51]\n",
    "        X=features\n",
    "        y=np.ravel(labels)\n",
    "        X_train, y_train = shuffle(X, y)\n",
    "        #print(X_train.shape)\n",
    "        \n",
    "        #Loading testing data\n",
    "        data12 = pd.read_csv(path_to_test,header=None)\n",
    "        labels12=data12.iloc[:,51]\n",
    "        features12 = data12.iloc[:,1:51]\n",
    "        X12=features12\n",
    "        y12=np.ravel(labels12)\n",
    "        X_test=X12;\n",
    "        y_test=y12;\n",
    "        #print(X_test.shape)\n",
    "        \n",
    "        # Making target data in range [0,9]\n",
    "        y_train=y_train-1;\n",
    "        y_test=y_test-1;\n",
    "        # Convert to categorical\n",
    "        Y_train = np_utils.to_categorical(y_train, myN) \n",
    "        Y_test = np_utils.to_categorical(y_test, myN)\n",
    "        Y_test.shape\n",
    "        \n",
    "        # Training model\n",
    "        checkpoint_path='d:/Data/Models/'+model_name+'.h5'\n",
    "        keras_callbacks   = [\n",
    "            EarlyStopping(monitor='val_loss', patience=50, verbose=1),\n",
    "            ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "        ]\n",
    "        \n",
    "        model=build_my_model(myN);\n",
    "        history=model.fit(X_train, Y_train,epochs=2000, batch_size=27, verbose=0, validation_split=0.1,callbacks=[keras_callbacks])\n",
    "        best_model = load_model(checkpoint_path)\n",
    "        model=best_model;\n",
    "        \n",
    "        # Calculate score\n",
    "        scores = model.evaluate(X_test, Y_test, verbose=2) \n",
    "        print(scores)\n",
    "        temp_results[i]=scores[1]       \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #1 Predicting Sub-dataset #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "12/12 - 0s - loss: 0.0208 - accuracy: 0.9894 - 85ms/epoch - 7ms/step\n",
      "[0.020801857113838196, 0.9893898963928223]\n",
      "Run N 2\n",
      "12/12 - 0s - loss: 0.0618 - accuracy: 0.9708 - 83ms/epoch - 7ms/step\n",
      "[0.06180218979716301, 0.970822274684906]\n",
      "Run N 3\n",
      "Epoch 01275: early stopping\n",
      "12/12 - 0s - loss: 0.0572 - accuracy: 0.9761 - 84ms/epoch - 7ms/step\n",
      "[0.057207729667425156, 0.9761273264884949]\n",
      "Run N 4\n",
      "12/12 - 0s - loss: 0.0517 - accuracy: 0.9814 - 83ms/epoch - 7ms/step\n",
      "[0.05170748382806778, 0.9814323782920837]\n",
      "Run N 5\n",
      "12/12 - 0s - loss: 0.0440 - accuracy: 0.9761 - 85ms/epoch - 7ms/step\n",
      "[0.043961819261312485, 0.9761273264884949]\n",
      "Run N 6\n",
      "12/12 - 0s - loss: 0.0600 - accuracy: 0.9708 - 95ms/epoch - 8ms/step\n",
      "[0.05999879166483879, 0.970822274684906]\n",
      "Run N 7\n",
      "12/12 - 0s - loss: 0.0511 - accuracy: 0.9788 - 86ms/epoch - 7ms/step\n",
      "[0.05113856494426727, 0.9787798523902893]\n",
      "Run N 8\n",
      "12/12 - 0s - loss: 0.0256 - accuracy: 0.9867 - 86ms/epoch - 7ms/step\n",
      "[0.025632163509726524, 0.9867374300956726]\n",
      "Run N 9\n",
      "12/12 - 0s - loss: 0.0701 - accuracy: 0.9735 - 86ms/epoch - 7ms/step\n",
      "[0.07005883753299713, 0.9734748005867004]\n",
      "Run N 10\n",
      "12/12 - 0s - loss: 0.0443 - accuracy: 0.9788 - 85ms/epoch - 7ms/step\n",
      "[0.04431185871362686, 0.9787798523902893]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"0/2500/Z_0_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"1/2500/Z_1_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_0_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 97.82%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #1 Predicting Sub-dataset #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "12/12 - 0s - loss: 0.3365 - accuracy: 0.9257 - 83ms/epoch - 7ms/step\n",
      "[0.336507648229599, 0.9257294535636902]\n",
      "Run N 2\n",
      "Epoch 00983: early stopping\n",
      "12/12 - 0s - loss: 0.3204 - accuracy: 0.8912 - 85ms/epoch - 7ms/step\n",
      "[0.3203907310962677, 0.8912466764450073]\n",
      "Run N 3\n",
      "12/12 - 0s - loss: 0.4544 - accuracy: 0.8939 - 385ms/epoch - 32ms/step\n",
      "[0.45435991883277893, 0.8938992023468018]\n",
      "Run N 4\n",
      "12/12 - 0s - loss: 0.3695 - accuracy: 0.9363 - 93ms/epoch - 8ms/step\n",
      "[0.3694707453250885, 0.9363394975662231]\n",
      "Run N 5\n",
      "Epoch 01162: early stopping\n",
      "12/12 - 0s - loss: 0.2586 - accuracy: 0.9337 - 84ms/epoch - 7ms/step\n",
      "[0.2585907280445099, 0.9336870312690735]\n",
      "Run N 6\n",
      "12/12 - 0s - loss: 0.2779 - accuracy: 0.9178 - 89ms/epoch - 7ms/step\n",
      "[0.2779076099395752, 0.9177718758583069]\n",
      "Run N 7\n",
      "Epoch 01336: early stopping\n",
      "12/12 - 0s - loss: 0.3112 - accuracy: 0.8966 - 89ms/epoch - 7ms/step\n",
      "[0.31122663617134094, 0.8965517282485962]\n",
      "Run N 8\n",
      "Epoch 01043: early stopping\n",
      "12/12 - 0s - loss: 0.3241 - accuracy: 0.9019 - 82ms/epoch - 7ms/step\n",
      "[0.32413822412490845, 0.9018567800521851]\n",
      "Run N 9\n",
      "Epoch 01947: early stopping\n",
      "12/12 - 0s - loss: 0.2821 - accuracy: 0.9443 - 85ms/epoch - 7ms/step\n",
      "[0.2821064293384552, 0.9442970752716064]\n",
      "Run N 10\n",
      "12/12 - 0s - loss: 0.3417 - accuracy: 0.9310 - 87ms/epoch - 7ms/step\n",
      "[0.34170904755592346, 0.931034505367279]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"0/2500/Z_0_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"2/2500/Z_2_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_0_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 91.72%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #1 Predicting Sub-dataset #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "12/12 - 0s - loss: 0.5446 - accuracy: 0.9151 - 85ms/epoch - 7ms/step\n",
      "[0.5445759296417236, 0.9151193499565125]\n",
      "Run N 2\n",
      "Epoch 01983: early stopping\n",
      "12/12 - 0s - loss: 0.5861 - accuracy: 0.8753 - 110ms/epoch - 9ms/step\n",
      "[0.5861058831214905, 0.8753315806388855]\n",
      "Run N 3\n",
      "12/12 - 0s - loss: 0.8188 - accuracy: 0.8727 - 90ms/epoch - 8ms/step\n",
      "[0.818821132183075, 0.8726790547370911]\n",
      "Run N 4\n",
      "12/12 - 0s - loss: 0.6595 - accuracy: 0.8886 - 105ms/epoch - 9ms/step\n",
      "[0.6594531536102295, 0.8885941505432129]\n",
      "Run N 5\n",
      "12/12 - 0s - loss: 0.7159 - accuracy: 0.8966 - 101ms/epoch - 8ms/step\n",
      "[0.715948760509491, 0.8965517282485962]\n",
      "Run N 6\n",
      "12/12 - 0s - loss: 0.5228 - accuracy: 0.9072 - 86ms/epoch - 7ms/step\n",
      "[0.5228347778320312, 0.9071618318557739]\n",
      "Run N 7\n",
      "Epoch 01467: early stopping\n",
      "12/12 - 0s - loss: 0.6558 - accuracy: 0.8780 - 80ms/epoch - 7ms/step\n",
      "[0.6558136343955994, 0.8779841065406799]\n",
      "Run N 8\n",
      "Epoch 01779: early stopping\n",
      "12/12 - 0s - loss: 0.7221 - accuracy: 0.8674 - 79ms/epoch - 7ms/step\n",
      "[0.7220911383628845, 0.8673740029335022]\n",
      "Run N 9\n",
      "Epoch 00882: early stopping\n",
      "12/12 - 0s - loss: 0.5499 - accuracy: 0.8594 - 85ms/epoch - 7ms/step\n",
      "[0.5499489903450012, 0.8594164252281189]\n",
      "Run N 10\n",
      "12/12 - 0s - loss: 0.6178 - accuracy: 0.8939 - 85ms/epoch - 7ms/step\n",
      "[0.6178405284881592, 0.8938992023468018]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"0/2500/Z_0_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"3/2500/Z_3_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_0_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 88.54%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #2 Predicting Sub-dataset #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "Epoch 01640: early stopping\n",
      "10/10 - 0s - loss: 0.0181 - accuracy: 0.9906 - 81ms/epoch - 8ms/step\n",
      "[0.018071115016937256, 0.9905956387519836]\n",
      "Run N 2\n",
      "Epoch 01978: early stopping\n",
      "10/10 - 0s - loss: 0.0123 - accuracy: 0.9969 - 75ms/epoch - 8ms/step\n",
      "[0.01229327917098999, 0.9968652129173279]\n",
      "Run N 3\n",
      "10/10 - 0s - loss: 0.0245 - accuracy: 0.9875 - 87ms/epoch - 9ms/step\n",
      "[0.024469628930091858, 0.9874607920646667]\n",
      "Run N 4\n",
      "Epoch 01627: early stopping\n",
      "10/10 - 0s - loss: 0.0234 - accuracy: 0.9906 - 86ms/epoch - 9ms/step\n",
      "[0.023372896015644073, 0.9905956387519836]\n",
      "Run N 5\n",
      "10/10 - 0s - loss: 0.0050 - accuracy: 0.9969 - 77ms/epoch - 8ms/step\n",
      "[0.005004251375794411, 0.9968652129173279]\n",
      "Run N 6\n",
      "Epoch 01751: early stopping\n",
      "10/10 - 0s - loss: 0.0089 - accuracy: 0.9937 - 92ms/epoch - 9ms/step\n",
      "[0.008856355212628841, 0.9937304258346558]\n",
      "Run N 7\n",
      "Epoch 01230: early stopping\n",
      "10/10 - 0s - loss: 0.0284 - accuracy: 0.9875 - 74ms/epoch - 7ms/step\n",
      "[0.028440939262509346, 0.9874607920646667]\n",
      "Run N 8\n",
      "10/10 - 0s - loss: 0.0092 - accuracy: 0.9937 - 75ms/epoch - 8ms/step\n",
      "[0.009205786511301994, 0.9937304258346558]\n",
      "Run N 9\n",
      "Epoch 01785: early stopping\n",
      "10/10 - 0s - loss: 0.0121 - accuracy: 0.9906 - 76ms/epoch - 8ms/step\n",
      "[0.012070323340594769, 0.9905956387519836]\n",
      "Run N 10\n",
      "Epoch 01956: early stopping\n",
      "10/10 - 0s - loss: 0.0053 - accuracy: 0.9969 - 89ms/epoch - 9ms/step\n",
      "[0.005270323716104031, 0.9968652129173279]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"1/2500/Z_1_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"0/2500/Z_0_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_1_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 99.25%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #2 Predicting Sub-dataset #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "Epoch 01905: early stopping\n",
      "12/12 - 0s - loss: 0.0398 - accuracy: 0.9920 - 78ms/epoch - 7ms/step\n",
      "[0.03981782868504524, 0.9920424222946167]\n",
      "Run N 2\n",
      "Epoch 01932: early stopping\n",
      "12/12 - 0s - loss: 0.0545 - accuracy: 0.9867 - 75ms/epoch - 6ms/step\n",
      "[0.05453768000006676, 0.9867374300956726]\n",
      "Run N 3\n",
      "Epoch 00680: early stopping\n",
      "12/12 - 0s - loss: 0.0537 - accuracy: 0.9920 - 76ms/epoch - 6ms/step\n",
      "[0.05372131988406181, 0.9920424222946167]\n",
      "Run N 4\n",
      "Epoch 01713: early stopping\n",
      "12/12 - 0s - loss: 0.0631 - accuracy: 0.9841 - 75ms/epoch - 6ms/step\n",
      "[0.06313131749629974, 0.9840849041938782]\n",
      "Run N 5\n",
      "Epoch 01787: early stopping\n",
      "12/12 - 0s - loss: 0.0588 - accuracy: 0.9788 - 86ms/epoch - 7ms/step\n",
      "[0.058750905096530914, 0.9787798523902893]\n",
      "Run N 6\n",
      "Epoch 01769: early stopping\n",
      "12/12 - 0s - loss: 0.0504 - accuracy: 0.9894 - 80ms/epoch - 7ms/step\n",
      "[0.050430405884981155, 0.9893898963928223]\n",
      "Run N 7\n",
      "Epoch 01163: early stopping\n",
      "12/12 - 0s - loss: 0.0526 - accuracy: 0.9788 - 88ms/epoch - 7ms/step\n",
      "[0.05259013921022415, 0.9787798523902893]\n",
      "Run N 8\n",
      "Epoch 01659: early stopping\n",
      "12/12 - 0s - loss: 0.0434 - accuracy: 0.9894 - 86ms/epoch - 7ms/step\n",
      "[0.04344350844621658, 0.9893898963928223]\n",
      "Run N 9\n",
      "Epoch 00660: early stopping\n",
      "12/12 - 0s - loss: 0.0720 - accuracy: 0.9761 - 83ms/epoch - 7ms/step\n",
      "[0.07195235788822174, 0.9761273264884949]\n",
      "Run N 10\n",
      "Epoch 00719: early stopping\n",
      "12/12 - 0s - loss: 0.0651 - accuracy: 0.9814 - 90ms/epoch - 8ms/step\n",
      "[0.06507246196269989, 0.9814323782920837]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"1/2500/Z_1_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"2/2500/Z_2_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_1_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 98.49%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #2 Predicting Sub-dataset #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "Epoch 01694: early stopping\n",
      "12/12 - 0s - loss: 0.2578 - accuracy: 0.9098 - 77ms/epoch - 6ms/step\n",
      "[0.2577957808971405, 0.9098142981529236]\n",
      "Run N 2\n",
      "Epoch 01636: early stopping\n",
      "12/12 - 0s - loss: 0.3308 - accuracy: 0.9045 - 76ms/epoch - 6ms/step\n",
      "[0.33082836866378784, 0.9045093059539795]\n",
      "Run N 3\n",
      "Epoch 01834: early stopping\n",
      "12/12 - 0s - loss: 0.2579 - accuracy: 0.9098 - 77ms/epoch - 6ms/step\n",
      "[0.25785505771636963, 0.9098142981529236]\n",
      "Run N 4\n",
      "Epoch 01701: early stopping\n",
      "12/12 - 0s - loss: 0.4731 - accuracy: 0.8753 - 85ms/epoch - 7ms/step\n",
      "[0.4731312692165375, 0.8753315806388855]\n",
      "Run N 5\n",
      "12/12 - 0s - loss: 0.2668 - accuracy: 0.9231 - 75ms/epoch - 6ms/step\n",
      "[0.26678356528282166, 0.9230769276618958]\n",
      "Run N 6\n",
      "Epoch 01675: early stopping\n",
      "12/12 - 0s - loss: 0.2776 - accuracy: 0.9045 - 75ms/epoch - 6ms/step\n",
      "[0.27761635184288025, 0.9045093059539795]\n",
      "Run N 7\n",
      "12/12 - 0s - loss: 0.2437 - accuracy: 0.9257 - 74ms/epoch - 6ms/step\n",
      "[0.2436695098876953, 0.9257294535636902]\n",
      "Run N 8\n",
      "12/12 - 0s - loss: 0.2257 - accuracy: 0.9257 - 79ms/epoch - 7ms/step\n",
      "[0.22574475407600403, 0.9257294535636902]\n",
      "Run N 9\n",
      "12/12 - 0s - loss: 0.2999 - accuracy: 0.9151 - 82ms/epoch - 7ms/step\n",
      "[0.2999247908592224, 0.9151193499565125]\n",
      "Run N 10\n",
      "Epoch 01338: early stopping\n",
      "12/12 - 0s - loss: 0.2625 - accuracy: 0.9125 - 79ms/epoch - 7ms/step\n",
      "[0.262482225894928, 0.912466824054718]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"1/2500/Z_1_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"3/2500/Z_3_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_1_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 91.06%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #3 Predicting Sub-dataset #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "Epoch 01404: early stopping\n",
      "10/10 - 0s - loss: 0.1305 - accuracy: 0.9498 - 82ms/epoch - 8ms/step\n",
      "[0.13051660358905792, 0.9498432874679565]\n",
      "Run N 2\n",
      "10/10 - 0s - loss: 0.1271 - accuracy: 0.9436 - 76ms/epoch - 8ms/step\n",
      "[0.12712518870830536, 0.9435736536979675]\n",
      "Run N 3\n",
      "10/10 - 0s - loss: 0.2115 - accuracy: 0.9310 - 85ms/epoch - 9ms/step\n",
      "[0.21145318448543549, 0.931034505367279]\n",
      "Run N 4\n",
      "Epoch 00996: early stopping\n",
      "10/10 - 0s - loss: 0.1421 - accuracy: 0.9530 - 78ms/epoch - 8ms/step\n",
      "[0.142074316740036, 0.9529780745506287]\n",
      "Run N 5\n",
      "10/10 - 0s - loss: 0.1846 - accuracy: 0.9279 - 82ms/epoch - 8ms/step\n",
      "[0.18458838760852814, 0.9278996586799622]\n",
      "Run N 6\n",
      "10/10 - 0s - loss: 0.1387 - accuracy: 0.9436 - 75ms/epoch - 8ms/step\n",
      "[0.13873730599880219, 0.9435736536979675]\n",
      "Run N 7\n",
      "Epoch 01935: early stopping\n",
      "10/10 - 0s - loss: 0.1860 - accuracy: 0.9248 - 77ms/epoch - 8ms/step\n",
      "[0.18599961698055267, 0.92476487159729]\n",
      "Run N 8\n",
      "10/10 - 0s - loss: 0.1708 - accuracy: 0.9310 - 84ms/epoch - 8ms/step\n",
      "[0.1708090901374817, 0.931034505367279]\n",
      "Run N 9\n",
      "Epoch 00836: early stopping\n",
      "10/10 - 0s - loss: 0.1874 - accuracy: 0.9404 - 84ms/epoch - 8ms/step\n",
      "[0.18743237853050232, 0.9404388666152954]\n",
      "Run N 10\n",
      "10/10 - 0s - loss: 0.1988 - accuracy: 0.9342 - 76ms/epoch - 8ms/step\n",
      "[0.1987726241350174, 0.9341692924499512]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"2/2500/Z_2_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"0/2500/Z_0_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_2_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 93.79%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #3 Predicting Sub-dataset #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "Epoch 01398: early stopping\n",
      "12/12 - 0s - loss: 0.0832 - accuracy: 0.9735 - 85ms/epoch - 7ms/step\n",
      "[0.0832420364022255, 0.9734748005867004]\n",
      "Run N 2\n",
      "12/12 - 0s - loss: 0.0661 - accuracy: 0.9814 - 78ms/epoch - 7ms/step\n",
      "[0.06605692952871323, 0.9814323782920837]\n",
      "Run N 3\n",
      "12/12 - 0s - loss: 0.0328 - accuracy: 0.9814 - 87ms/epoch - 7ms/step\n",
      "[0.032814521342515945, 0.9814323782920837]\n",
      "Run N 4\n",
      "12/12 - 0s - loss: 0.0595 - accuracy: 0.9788 - 84ms/epoch - 7ms/step\n",
      "[0.059480976313352585, 0.9787798523902893]\n",
      "Run N 5\n",
      "12/12 - 0s - loss: 0.1435 - accuracy: 0.9655 - 76ms/epoch - 6ms/step\n",
      "[0.1435062140226364, 0.9655172228813171]\n",
      "Run N 6\n",
      "12/12 - 0s - loss: 0.0423 - accuracy: 0.9841 - 85ms/epoch - 7ms/step\n",
      "[0.042345382273197174, 0.9840849041938782]\n",
      "Run N 7\n",
      "12/12 - 0s - loss: 0.0711 - accuracy: 0.9735 - 80ms/epoch - 7ms/step\n",
      "[0.07109255343675613, 0.9734748005867004]\n",
      "Run N 8\n",
      "Epoch 01278: early stopping\n",
      "12/12 - 0s - loss: 0.0759 - accuracy: 0.9708 - 86ms/epoch - 7ms/step\n",
      "[0.07588699460029602, 0.970822274684906]\n",
      "Run N 9\n",
      "12/12 - 0s - loss: 0.0334 - accuracy: 0.9867 - 85ms/epoch - 7ms/step\n",
      "[0.03342300280928612, 0.9867374300956726]\n",
      "Run N 10\n",
      "12/12 - 0s - loss: 0.0528 - accuracy: 0.9814 - 84ms/epoch - 7ms/step\n",
      "[0.05283317342400551, 0.9814323782920837]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"2/2500/Z_2_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"1/2500/Z_1_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_2_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 97.77%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #3 Predicting Sub-dataset #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "12/12 - 0s - loss: 0.0868 - accuracy: 0.9708 - 87ms/epoch - 7ms/step\n",
      "[0.08684483170509338, 0.970822274684906]\n",
      "Run N 2\n",
      "Epoch 01302: early stopping\n",
      "12/12 - 0s - loss: 0.1101 - accuracy: 0.9682 - 84ms/epoch - 7ms/step\n",
      "[0.11012500524520874, 0.9681697487831116]\n",
      "Run N 3\n",
      "Epoch 01135: early stopping\n",
      "12/12 - 0s - loss: 0.0753 - accuracy: 0.9682 - 78ms/epoch - 7ms/step\n",
      "[0.07527798414230347, 0.9681697487831116]\n",
      "Run N 4\n",
      "12/12 - 0s - loss: 0.1010 - accuracy: 0.9682 - 93ms/epoch - 8ms/step\n",
      "[0.10103824734687805, 0.9681697487831116]\n",
      "Run N 5\n",
      "12/12 - 0s - loss: 0.0808 - accuracy: 0.9708 - 76ms/epoch - 6ms/step\n",
      "[0.08082086592912674, 0.970822274684906]\n",
      "Run N 6\n",
      "12/12 - 0s - loss: 0.1402 - accuracy: 0.9655 - 88ms/epoch - 7ms/step\n",
      "[0.14018981158733368, 0.9655172228813171]\n",
      "Run N 7\n",
      "12/12 - 0s - loss: 0.0450 - accuracy: 0.9788 - 81ms/epoch - 7ms/step\n",
      "[0.04501829296350479, 0.9787798523902893]\n",
      "Run N 8\n",
      "12/12 - 0s - loss: 0.0999 - accuracy: 0.9682 - 84ms/epoch - 7ms/step\n",
      "[0.09988591820001602, 0.9681697487831116]\n",
      "Run N 9\n",
      "12/12 - 0s - loss: 0.0678 - accuracy: 0.9682 - 77ms/epoch - 6ms/step\n",
      "[0.06776104122400284, 0.9681697487831116]\n",
      "Run N 10\n",
      "12/12 - 0s - loss: 0.1025 - accuracy: 0.9682 - 85ms/epoch - 7ms/step\n",
      "[0.10245270282030106, 0.9681697487831116]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"2/2500/Z_2_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"3/2500/Z_3_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_2_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 96.95%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #4 Predicting Sub-dataset #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "10/10 - 0s - loss: 0.4016 - accuracy: 0.8370 - 79ms/epoch - 8ms/step\n",
      "[0.4015938341617584, 0.8369905948638916]\n",
      "Run N 2\n",
      "Epoch 01225: early stopping\n",
      "10/10 - 0s - loss: 0.5342 - accuracy: 0.8245 - 83ms/epoch - 8ms/step\n",
      "[0.5341582894325256, 0.8244513869285583]\n",
      "Run N 3\n",
      "Epoch 01303: early stopping\n",
      "10/10 - 0s - loss: 0.4629 - accuracy: 0.8464 - 86ms/epoch - 9ms/step\n",
      "[0.46290019154548645, 0.846394956111908]\n",
      "Run N 4\n",
      "Epoch 01027: early stopping\n",
      "10/10 - 0s - loss: 0.4660 - accuracy: 0.8276 - 85ms/epoch - 9ms/step\n",
      "[0.4659697413444519, 0.8275862336158752]\n",
      "Run N 5\n",
      "Epoch 01663: early stopping\n",
      "10/10 - 0s - loss: 0.4433 - accuracy: 0.8370 - 84ms/epoch - 8ms/step\n",
      "[0.4432924687862396, 0.8369905948638916]\n",
      "Run N 6\n",
      "Epoch 00901: early stopping\n",
      "10/10 - 0s - loss: 0.3464 - accuracy: 0.8746 - 79ms/epoch - 8ms/step\n",
      "[0.34644508361816406, 0.8746081590652466]\n",
      "Run N 7\n",
      "Epoch 01912: early stopping\n",
      "10/10 - 0s - loss: 0.6526 - accuracy: 0.8150 - 76ms/epoch - 8ms/step\n",
      "[0.6526158452033997, 0.815047025680542]\n",
      "Run N 8\n",
      "Epoch 01222: early stopping\n",
      "10/10 - 0s - loss: 0.5439 - accuracy: 0.8245 - 76ms/epoch - 8ms/step\n",
      "[0.5438541769981384, 0.8244513869285583]\n",
      "Run N 9\n",
      "Epoch 01439: early stopping\n",
      "10/10 - 0s - loss: 0.4742 - accuracy: 0.8433 - 79ms/epoch - 8ms/step\n",
      "[0.47423961758613586, 0.8432601690292358]\n",
      "Run N 10\n",
      "Epoch 01360: early stopping\n",
      "10/10 - 0s - loss: 0.4238 - accuracy: 0.8527 - 76ms/epoch - 8ms/step\n",
      "[0.42377278208732605, 0.852664589881897]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"3/2500/Z_3_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"0/2500/Z_0_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_3_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 83.82%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #4 Predicting Sub-dataset #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "Epoch 01799: early stopping\n",
      "12/12 - 0s - loss: 0.2026 - accuracy: 0.9151 - 75ms/epoch - 6ms/step\n",
      "[0.20258331298828125, 0.9151193499565125]\n",
      "Run N 2\n",
      "12/12 - 0s - loss: 0.1730 - accuracy: 0.9363 - 85ms/epoch - 7ms/step\n",
      "[0.17298413813114166, 0.9363394975662231]\n",
      "Run N 3\n",
      "Epoch 01155: early stopping\n",
      "12/12 - 0s - loss: 0.2127 - accuracy: 0.9469 - 84ms/epoch - 7ms/step\n",
      "[0.2126733362674713, 0.9469496011734009]\n",
      "Run N 4\n",
      "Epoch 01425: early stopping\n",
      "12/12 - 0s - loss: 0.2163 - accuracy: 0.9337 - 90ms/epoch - 8ms/step\n",
      "[0.21625134348869324, 0.9336870312690735]\n",
      "Run N 5\n",
      "12/12 - 0s - loss: 0.3260 - accuracy: 0.9284 - 80ms/epoch - 7ms/step\n",
      "[0.32597535848617554, 0.9283819794654846]\n",
      "Run N 6\n",
      "12/12 - 0s - loss: 0.1945 - accuracy: 0.9284 - 79ms/epoch - 7ms/step\n",
      "[0.19448964297771454, 0.9283819794654846]\n",
      "Run N 7\n",
      "Epoch 01648: early stopping\n",
      "12/12 - 0s - loss: 0.1796 - accuracy: 0.9390 - 81ms/epoch - 7ms/step\n",
      "[0.17963409423828125, 0.9389920234680176]\n",
      "Run N 8\n",
      "12/12 - 0s - loss: 0.1826 - accuracy: 0.9416 - 91ms/epoch - 8ms/step\n",
      "[0.18257735669612885, 0.941644549369812]\n",
      "Run N 9\n",
      "12/12 - 0s - loss: 0.2296 - accuracy: 0.9310 - 81ms/epoch - 7ms/step\n",
      "[0.2295941859483719, 0.931034505367279]\n",
      "Run N 10\n",
      "12/12 - 0s - loss: 0.1895 - accuracy: 0.9363 - 88ms/epoch - 7ms/step\n",
      "[0.18951460719108582, 0.9363394975662231]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"3/2500/Z_3_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"1/2500/Z_1_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_3_1_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 93.37%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #4 Predicting Sub-dataset #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "12/12 - 0s - loss: 0.0203 - accuracy: 0.9947 - 84ms/epoch - 7ms/step\n",
      "[0.020328689366579056, 0.9946949481964111]\n",
      "Run N 2\n",
      "Epoch 01903: early stopping\n",
      "12/12 - 0s - loss: 0.0252 - accuracy: 0.9920 - 76ms/epoch - 6ms/step\n",
      "[0.02523549273610115, 0.9920424222946167]\n",
      "Run N 3\n",
      "Epoch 01281: early stopping\n",
      "12/12 - 0s - loss: 0.1228 - accuracy: 0.9629 - 87ms/epoch - 7ms/step\n",
      "[0.12283175438642502, 0.9628646969795227]\n",
      "Run N 4\n",
      "12/12 - 0s - loss: 0.0328 - accuracy: 0.9894 - 77ms/epoch - 6ms/step\n",
      "[0.03277912363409996, 0.9893898963928223]\n",
      "Run N 5\n",
      "Epoch 01718: early stopping\n",
      "12/12 - 0s - loss: 0.0431 - accuracy: 0.9814 - 77ms/epoch - 6ms/step\n",
      "[0.04309648647904396, 0.9814323782920837]\n",
      "Run N 6\n",
      "Epoch 01476: early stopping\n",
      "12/12 - 0s - loss: 0.0725 - accuracy: 0.9788 - 84ms/epoch - 7ms/step\n",
      "[0.07249314337968826, 0.9787798523902893]\n",
      "Run N 7\n",
      "Epoch 01633: early stopping\n",
      "12/12 - 0s - loss: 0.1042 - accuracy: 0.9735 - 87ms/epoch - 7ms/step\n",
      "[0.10424976795911789, 0.9734748005867004]\n",
      "Run N 8\n",
      "Epoch 01480: early stopping\n",
      "12/12 - 0s - loss: 0.0595 - accuracy: 0.9814 - 75ms/epoch - 6ms/step\n",
      "[0.059476710855960846, 0.9814323782920837]\n",
      "Run N 9\n",
      "12/12 - 0s - loss: 0.0552 - accuracy: 0.9814 - 87ms/epoch - 7ms/step\n",
      "[0.05521067976951599, 0.9814323782920837]\n",
      "Run N 10\n",
      "Epoch 01417: early stopping\n",
      "12/12 - 0s - loss: 0.0763 - accuracy: 0.9788 - 77ms/epoch - 6ms/step\n",
      "[0.07627373188734055, 0.9787798523902893]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"3/2500/Z_3_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"2/2500/Z_2_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_3_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 98.14%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Entire dataset  Predicting Sub-dataset #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "Epoch 00505: early stopping\n",
      "10/10 - 0s - loss: 2.2377e-04 - accuracy: 1.0000 - 78ms/epoch - 8ms/step\n",
      "[0.00022377294953912497, 1.0]\n",
      "Run N 2\n",
      "Epoch 01761: early stopping\n",
      "10/10 - 0s - loss: 2.6669e-06 - accuracy: 1.0000 - 84ms/epoch - 8ms/step\n",
      "[2.666907903403626e-06, 1.0]\n",
      "Run N 3\n",
      "Epoch 01534: early stopping\n",
      "10/10 - 0s - loss: 1.0773e-06 - accuracy: 1.0000 - 84ms/epoch - 8ms/step\n",
      "[1.0773251233331393e-06, 1.0]\n",
      "Run N 4\n",
      "Epoch 01036: early stopping\n",
      "10/10 - 0s - loss: 9.3589e-06 - accuracy: 1.0000 - 83ms/epoch - 8ms/step\n",
      "[9.358914212498348e-06, 1.0]\n",
      "Run N 5\n",
      "Epoch 00503: early stopping\n",
      "10/10 - 0s - loss: 2.2671e-04 - accuracy: 1.0000 - 75ms/epoch - 8ms/step\n",
      "[0.0002267098898300901, 1.0]\n",
      "Run N 6\n",
      "Epoch 01070: early stopping\n",
      "10/10 - 0s - loss: 2.6052e-05 - accuracy: 1.0000 - 81ms/epoch - 8ms/step\n",
      "[2.605205554573331e-05, 1.0]\n",
      "Run N 7\n",
      "Epoch 00509: early stopping\n",
      "10/10 - 0s - loss: 3.3392e-04 - accuracy: 1.0000 - 83ms/epoch - 8ms/step\n",
      "[0.0003339189279358834, 1.0]\n",
      "Run N 8\n",
      "Epoch 01593: early stopping\n",
      "10/10 - 0s - loss: 2.2066e-06 - accuracy: 1.0000 - 84ms/epoch - 8ms/step\n",
      "[2.206576255048276e-06, 1.0]\n",
      "Run N 9\n",
      "Epoch 00688: early stopping\n",
      "10/10 - 0s - loss: 4.1355e-05 - accuracy: 1.0000 - 75ms/epoch - 8ms/step\n",
      "[4.135549897910096e-05, 1.0]\n",
      "Run N 10\n",
      "Epoch 00835: early stopping\n",
      "10/10 - 0s - loss: 1.1656e-05 - accuracy: 1.0000 - 86ms/epoch - 9ms/step\n",
      "[1.1655881280603353e-05, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"ALL/2500/Z_ALL_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"0/2500/Z_0_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_ALL_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Entire dataset  Predicting Sub-dataset #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "Epoch 01637: early stopping\n",
      "12/12 - 0s - loss: 3.8173e-06 - accuracy: 1.0000 - 75ms/epoch - 6ms/step\n",
      "[3.8173307075339835e-06, 1.0]\n",
      "Run N 2\n",
      "Epoch 01326: early stopping\n",
      "12/12 - 0s - loss: 3.4384e-06 - accuracy: 1.0000 - 81ms/epoch - 7ms/step\n",
      "[3.4384495393169345e-06, 1.0]\n",
      "Run N 3\n",
      "Epoch 01294: early stopping\n",
      "12/12 - 0s - loss: 1.1021e-05 - accuracy: 1.0000 - 84ms/epoch - 7ms/step\n",
      "[1.1020943929906934e-05, 1.0]\n",
      "Run N 4\n",
      "Epoch 00877: early stopping\n",
      "12/12 - 0s - loss: 2.1494e-05 - accuracy: 1.0000 - 80ms/epoch - 7ms/step\n",
      "[2.14939791476354e-05, 1.0]\n",
      "Run N 5\n",
      "Epoch 01508: early stopping\n",
      "12/12 - 0s - loss: 1.4068e-05 - accuracy: 1.0000 - 79ms/epoch - 7ms/step\n",
      "[1.4067717529542278e-05, 1.0]\n",
      "Run N 6\n",
      "Epoch 01472: early stopping\n",
      "12/12 - 0s - loss: 7.4925e-06 - accuracy: 1.0000 - 81ms/epoch - 7ms/step\n",
      "[7.492504209949402e-06, 1.0]\n",
      "Run N 7\n",
      "Epoch 01032: early stopping\n",
      "12/12 - 0s - loss: 1.8544e-05 - accuracy: 1.0000 - 80ms/epoch - 7ms/step\n",
      "[1.854439324233681e-05, 1.0]\n",
      "Run N 8\n",
      "Epoch 01157: early stopping\n",
      "12/12 - 0s - loss: 1.2306e-05 - accuracy: 1.0000 - 77ms/epoch - 6ms/step\n",
      "[1.2305779819143936e-05, 1.0]\n",
      "Run N 9\n",
      "Epoch 00859: early stopping\n",
      "12/12 - 0s - loss: 4.5215e-05 - accuracy: 1.0000 - 85ms/epoch - 7ms/step\n",
      "[4.521527444012463e-05, 1.0]\n",
      "Run N 10\n",
      "12/12 - 0s - loss: 6.6614e-06 - accuracy: 1.0000 - 88ms/epoch - 7ms/step\n",
      "[6.661410225206055e-06, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"ALL/2500/Z_ALL_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"1/2500/Z_1_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_ALL_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Entire dataset  Predicting Sub-dataset #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "Epoch 00665: early stopping\n",
      "12/12 - 0s - loss: 3.9375e-05 - accuracy: 1.0000 - 84ms/epoch - 7ms/step\n",
      "[3.937503424822353e-05, 1.0]\n",
      "Run N 2\n",
      "Epoch 00885: early stopping\n",
      "12/12 - 0s - loss: 7.2366e-06 - accuracy: 1.0000 - 81ms/epoch - 7ms/step\n",
      "[7.236603778437711e-06, 1.0]\n",
      "Run N 3\n",
      "Epoch 00562: early stopping\n",
      "12/12 - 0s - loss: 3.3113e-05 - accuracy: 1.0000 - 77ms/epoch - 6ms/step\n",
      "[3.31129995174706e-05, 1.0]\n",
      "Run N 4\n",
      "12/12 - 0s - loss: 3.9619e-07 - accuracy: 1.0000 - 74ms/epoch - 6ms/step\n",
      "[3.9618936398255755e-07, 1.0]\n",
      "Run N 5\n",
      "Epoch 00912: early stopping\n",
      "12/12 - 0s - loss: 1.5060e-05 - accuracy: 1.0000 - 88ms/epoch - 7ms/step\n",
      "[1.5059634279168677e-05, 1.0]\n",
      "Run N 6\n",
      "Epoch 00997: early stopping\n",
      "12/12 - 0s - loss: 4.9890e-06 - accuracy: 1.0000 - 78ms/epoch - 7ms/step\n",
      "[4.98896770295687e-06, 1.0]\n",
      "Run N 7\n",
      "Epoch 01283: early stopping\n",
      "12/12 - 0s - loss: 5.4122e-06 - accuracy: 1.0000 - 78ms/epoch - 7ms/step\n",
      "[5.412182417785516e-06, 1.0]\n",
      "Run N 8\n",
      "Epoch 00806: early stopping\n",
      "12/12 - 0s - loss: 8.1113e-06 - accuracy: 1.0000 - 88ms/epoch - 7ms/step\n",
      "[8.111260285659228e-06, 1.0]\n",
      "Run N 9\n",
      "Epoch 00651: early stopping\n",
      "12/12 - 0s - loss: 3.7515e-05 - accuracy: 1.0000 - 81ms/epoch - 7ms/step\n",
      "[3.751515396288596e-05, 1.0]\n",
      "Run N 10\n",
      "Epoch 01207: early stopping\n",
      "12/12 - 0s - loss: 1.2834e-06 - accuracy: 1.0000 - 82ms/epoch - 7ms/step\n",
      "[1.2833613709517522e-06, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"ALL/2500/Z_ALL_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"2/2500/Z_2_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_ALL_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Entire dataset  Predicting Sub-dataset #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run N 1\n",
      "Epoch 01924: early stopping\n",
      "12/12 - 0s - loss: 3.3350e-06 - accuracy: 1.0000 - 81ms/epoch - 7ms/step\n",
      "[3.3350204375892645e-06, 1.0]\n",
      "Run N 2\n",
      "Epoch 01182: early stopping\n",
      "12/12 - 0s - loss: 6.9342e-06 - accuracy: 1.0000 - 74ms/epoch - 6ms/step\n",
      "[6.9341531343525276e-06, 1.0]\n",
      "Run N 3\n",
      "Epoch 00957: early stopping\n",
      "12/12 - 0s - loss: 2.6141e-05 - accuracy: 1.0000 - 74ms/epoch - 6ms/step\n",
      "[2.614052027638536e-05, 1.0]\n",
      "Run N 4\n",
      "Epoch 01229: early stopping\n",
      "12/12 - 0s - loss: 1.3888e-05 - accuracy: 1.0000 - 77ms/epoch - 6ms/step\n",
      "[1.388775126542896e-05, 1.0]\n",
      "Run N 5\n",
      "Epoch 00710: early stopping\n",
      "12/12 - 0s - loss: 5.5985e-05 - accuracy: 1.0000 - 73ms/epoch - 6ms/step\n",
      "[5.598537245532498e-05, 1.0]\n",
      "Run N 6\n",
      "Epoch 01172: early stopping\n",
      "12/12 - 0s - loss: 1.6065e-05 - accuracy: 1.0000 - 76ms/epoch - 6ms/step\n",
      "[1.6065148884081282e-05, 1.0]\n",
      "Run N 7\n",
      "Epoch 01377: early stopping\n",
      "12/12 - 0s - loss: 3.6868e-06 - accuracy: 1.0000 - 82ms/epoch - 7ms/step\n",
      "[3.6868009374302346e-06, 1.0]\n",
      "Run N 8\n",
      "Epoch 00743: early stopping\n",
      "12/12 - 0s - loss: 6.3945e-05 - accuracy: 1.0000 - 83ms/epoch - 7ms/step\n",
      "[6.394497177097946e-05, 1.0]\n",
      "Run N 9\n",
      "Epoch 01366: early stopping\n",
      "12/12 - 0s - loss: 8.2478e-06 - accuracy: 1.0000 - 83ms/epoch - 7ms/step\n",
      "[8.247813639172819e-06, 1.0]\n",
      "Run N 10\n",
      "Epoch 00685: early stopping\n",
      "12/12 - 0s - loss: 0.0011 - accuracy: 1.0000 - 78ms/epoch - 7ms/step\n",
      "[0.001065734657458961, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"ALL/2500/Z_ALL_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"3/2500/Z_3_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(number_Of_Trials)\n",
    "\n",
    "training(myN, number_Of_Trials, 'bestmodel_ALL_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sub-dataset #1,2,3  Predicting Sub-dataset #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0        1       2        3        4        5        6        7   \\\n",
      "51                                                                         \n",
      "1  0     1 -1.77810  1.1086 -0.33531  0.36896 -0.62876  0.79064 -0.77906   \n",
      "   1     1 -2.05680  1.8892 -1.38440  1.44500 -1.78190  2.08580 -2.27030   \n",
      "   2     1 -1.87120  1.3458 -0.64200  0.73745 -1.03260  1.16570 -1.16600   \n",
      "   3     1 -2.02190  1.7031 -1.05500  1.05490 -1.37290  1.62750 -1.69200   \n",
      "   4     1 -1.89890  1.4202 -0.72660  0.70561 -0.93685  1.12400 -1.16340   \n",
      "...     ..      ...     ...      ...      ...      ...      ...      ...   \n",
      "10 198   1 -0.89360  1.8898 -2.04120  2.11070 -2.00790  2.30120 -1.96840   \n",
      "   199   1 -0.73466  1.7159 -1.87830  1.92880 -1.96130  2.34100 -2.06070   \n",
      "   200   1 -0.94103  1.9364 -2.10460  2.10460 -1.99700  2.11860 -1.73990   \n",
      "   201   1 -0.68984  1.5191 -1.53520  1.39650 -1.25210  1.56860 -1.10860   \n",
      "   202   1 -0.95087  1.9418 -2.18430  2.29250 -2.25160  2.57860 -2.31140   \n",
      "\n",
      "             8        9   ...        42        43        44        45  \\\n",
      "51                        ...                                           \n",
      "1  0    0.53082 -0.23105  ...  0.129570 -0.121560  0.064142 -0.033289   \n",
      "   1    2.17120 -1.84380  ...  0.036165  0.018983 -0.046387 -0.032013   \n",
      "   2    0.98197 -0.67322  ...  0.028606  0.005777 -0.003736 -0.095507   \n",
      "   3    1.50380 -1.16170  ... -0.019667 -0.046114  0.029361 -0.005795   \n",
      "   4    0.92526 -0.54792  ... -0.068251  0.110360 -0.090378  0.016394   \n",
      "...         ...      ...  ...       ...       ...       ...       ...   \n",
      "10 198  2.50000 -2.63320  ...  0.587060 -0.549210  0.347410 -0.314950   \n",
      "   199  2.64840 -2.77980  ...  0.792990 -0.714630  0.517130 -0.449140   \n",
      "   200  2.10350 -2.08710  ...  0.081975 -0.158810  0.016654 -0.050297   \n",
      "   201  1.54100 -1.47870  ... -0.154660  0.039840 -0.118110  0.061164   \n",
      "   202  2.81060 -2.99200  ...  0.709670 -0.638080  0.446600 -0.368410   \n",
      "\n",
      "              46        47        48        49        50  51  \n",
      "51                                                            \n",
      "1  0    0.009829  0.113900 -0.153120  0.056293 -0.002588   1  \n",
      "   1    0.135220 -0.020270 -0.079704  0.033162  0.012507   1  \n",
      "   2    0.172690 -0.020604 -0.092462  0.024970  0.019400   1  \n",
      "   3    0.032369  0.063086 -0.072533 -0.017732  0.043692   1  \n",
      "   4    0.044181  0.082628 -0.119130  0.002906  0.036579   1  \n",
      "...          ...       ...       ...       ...       ...  ..  \n",
      "10 198  0.231930 -0.184370  0.082354 -0.072951  0.012487  10  \n",
      "   199  0.341530 -0.225430  0.117120 -0.096263  0.001205  10  \n",
      "   200  0.018560  0.024656 -0.060932  0.020774 -0.048105  10  \n",
      "   201 -0.038611  0.033378 -0.059330 -0.012138 -0.025902  10  \n",
      "   202  0.290120 -0.215390  0.109630 -0.079300  0.026433  10  \n",
      "\n",
      "[2030 rows x 52 columns]\n",
      "(2030, 50)\n",
      "(377, 50)\n",
      "Epoch 1/2000\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 2.7092 - accuracy: 0.0909 - val_loss: 2.6088 - val_accuracy: 0.0493\n",
      "Epoch 2/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.4624 - accuracy: 0.0394 - val_loss: 2.4381 - val_accuracy: 0.0099\n",
      "Epoch 3/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.3210 - accuracy: 0.0328 - val_loss: 2.3106 - val_accuracy: 0.0246\n",
      "Epoch 4/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.2157 - accuracy: 0.0865 - val_loss: 2.2013 - val_accuracy: 0.1084\n",
      "Epoch 5/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.1338 - accuracy: 0.1412 - val_loss: 2.1220 - val_accuracy: 0.1527\n",
      "Epoch 6/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.0735 - accuracy: 0.1910 - val_loss: 2.0609 - val_accuracy: 0.2217\n",
      "Epoch 7/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.0233 - accuracy: 0.2299 - val_loss: 2.0146 - val_accuracy: 0.2562\n",
      "Epoch 8/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.9780 - accuracy: 0.2753 - val_loss: 1.9679 - val_accuracy: 0.2956\n",
      "Epoch 9/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.9353 - accuracy: 0.3459 - val_loss: 1.9283 - val_accuracy: 0.3596\n",
      "Epoch 10/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.8950 - accuracy: 0.4302 - val_loss: 1.8897 - val_accuracy: 0.4335\n",
      "Epoch 11/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.8560 - accuracy: 0.5036 - val_loss: 1.8533 - val_accuracy: 0.4680\n",
      "Epoch 12/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.8187 - accuracy: 0.5468 - val_loss: 1.8151 - val_accuracy: 0.5172\n",
      "Epoch 13/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.7819 - accuracy: 0.5796 - val_loss: 1.7807 - val_accuracy: 0.5419\n",
      "Epoch 14/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.7467 - accuracy: 0.5933 - val_loss: 1.7456 - val_accuracy: 0.5517\n",
      "Epoch 15/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.7120 - accuracy: 0.5999 - val_loss: 1.7107 - val_accuracy: 0.5616\n",
      "Epoch 16/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.6781 - accuracy: 0.6125 - val_loss: 1.6776 - val_accuracy: 0.5714\n",
      "Epoch 17/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.6450 - accuracy: 0.6229 - val_loss: 1.6463 - val_accuracy: 0.5961\n",
      "Epoch 18/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.6125 - accuracy: 0.6355 - val_loss: 1.6154 - val_accuracy: 0.6059\n",
      "Epoch 19/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.5818 - accuracy: 0.6409 - val_loss: 1.5856 - val_accuracy: 0.6207\n",
      "Epoch 20/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.5522 - accuracy: 0.6530 - val_loss: 1.5561 - val_accuracy: 0.6502\n",
      "Epoch 21/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.5233 - accuracy: 0.6804 - val_loss: 1.5242 - val_accuracy: 0.6502\n",
      "Epoch 22/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.4947 - accuracy: 0.6875 - val_loss: 1.4990 - val_accuracy: 0.6749\n",
      "Epoch 23/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.4675 - accuracy: 0.6869 - val_loss: 1.4670 - val_accuracy: 0.6847\n",
      "Epoch 24/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.4400 - accuracy: 0.7110 - val_loss: 1.4398 - val_accuracy: 0.7094\n",
      "Epoch 25/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.4129 - accuracy: 0.7132 - val_loss: 1.4134 - val_accuracy: 0.7094\n",
      "Epoch 26/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.3858 - accuracy: 0.7209 - val_loss: 1.3870 - val_accuracy: 0.7291\n",
      "Epoch 27/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.3592 - accuracy: 0.7378 - val_loss: 1.3574 - val_accuracy: 0.7488\n",
      "Epoch 28/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.3324 - accuracy: 0.7373 - val_loss: 1.3285 - val_accuracy: 0.7537\n",
      "Epoch 29/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.3054 - accuracy: 0.7526 - val_loss: 1.3001 - val_accuracy: 0.7586\n",
      "Epoch 30/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.2790 - accuracy: 0.7646 - val_loss: 1.2750 - val_accuracy: 0.7783\n",
      "Epoch 31/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.2530 - accuracy: 0.7723 - val_loss: 1.2501 - val_accuracy: 0.7783\n",
      "Epoch 32/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.2276 - accuracy: 0.7778 - val_loss: 1.2241 - val_accuracy: 0.7980\n",
      "Epoch 33/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.2034 - accuracy: 0.7843 - val_loss: 1.1983 - val_accuracy: 0.8030\n",
      "Epoch 34/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.1792 - accuracy: 0.7898 - val_loss: 1.1743 - val_accuracy: 0.8079\n",
      "Epoch 35/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.1549 - accuracy: 0.8008 - val_loss: 1.1516 - val_accuracy: 0.8276\n",
      "Epoch 36/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.1316 - accuracy: 0.8084 - val_loss: 1.1279 - val_accuracy: 0.8276\n",
      "Epoch 37/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.1085 - accuracy: 0.8144 - val_loss: 1.1052 - val_accuracy: 0.8325\n",
      "Epoch 38/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.0858 - accuracy: 0.8205 - val_loss: 1.0784 - val_accuracy: 0.8522\n",
      "Epoch 39/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.0630 - accuracy: 0.8281 - val_loss: 1.0561 - val_accuracy: 0.8571\n",
      "Epoch 40/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.0415 - accuracy: 0.8276 - val_loss: 1.0370 - val_accuracy: 0.8571\n",
      "Epoch 41/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.0200 - accuracy: 0.8358 - val_loss: 1.0137 - val_accuracy: 0.8621\n",
      "Epoch 42/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9983 - accuracy: 0.8429 - val_loss: 0.9892 - val_accuracy: 0.8621\n",
      "Epoch 43/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9774 - accuracy: 0.8440 - val_loss: 0.9705 - val_accuracy: 0.8621\n",
      "Epoch 44/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9566 - accuracy: 0.8511 - val_loss: 0.9482 - val_accuracy: 0.8719\n",
      "Epoch 45/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9362 - accuracy: 0.8539 - val_loss: 0.9260 - val_accuracy: 0.8719\n",
      "Epoch 46/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9166 - accuracy: 0.8555 - val_loss: 0.9092 - val_accuracy: 0.8768\n",
      "Epoch 47/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8971 - accuracy: 0.8604 - val_loss: 0.8881 - val_accuracy: 0.8768\n",
      "Epoch 48/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8776 - accuracy: 0.8648 - val_loss: 0.8662 - val_accuracy: 0.8768\n",
      "Epoch 49/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8589 - accuracy: 0.8692 - val_loss: 0.8454 - val_accuracy: 0.8818\n",
      "Epoch 50/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8404 - accuracy: 0.8730 - val_loss: 0.8303 - val_accuracy: 0.8867\n",
      "Epoch 51/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8216 - accuracy: 0.8752 - val_loss: 0.8100 - val_accuracy: 0.8966\n",
      "Epoch 52/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8043 - accuracy: 0.8812 - val_loss: 0.7955 - val_accuracy: 0.8966\n",
      "Epoch 53/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7866 - accuracy: 0.8834 - val_loss: 0.7753 - val_accuracy: 0.9064\n",
      "Epoch 54/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7693 - accuracy: 0.8862 - val_loss: 0.7607 - val_accuracy: 0.9015\n",
      "Epoch 55/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7524 - accuracy: 0.8922 - val_loss: 0.7422 - val_accuracy: 0.9064\n",
      "Epoch 56/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7360 - accuracy: 0.8960 - val_loss: 0.7236 - val_accuracy: 0.9113\n",
      "Epoch 57/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7196 - accuracy: 0.8987 - val_loss: 0.7072 - val_accuracy: 0.9113\n",
      "Epoch 58/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7041 - accuracy: 0.9015 - val_loss: 0.6959 - val_accuracy: 0.9212\n",
      "Epoch 59/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6886 - accuracy: 0.9080 - val_loss: 0.6799 - val_accuracy: 0.9212\n",
      "Epoch 60/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6740 - accuracy: 0.9146 - val_loss: 0.6629 - val_accuracy: 0.9360\n",
      "Epoch 61/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6588 - accuracy: 0.9168 - val_loss: 0.6468 - val_accuracy: 0.9360\n",
      "Epoch 62/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6439 - accuracy: 0.9217 - val_loss: 0.6303 - val_accuracy: 0.9360\n",
      "Epoch 63/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6299 - accuracy: 0.9256 - val_loss: 0.6205 - val_accuracy: 0.9360\n",
      "Epoch 64/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6158 - accuracy: 0.9261 - val_loss: 0.6019 - val_accuracy: 0.9409\n",
      "Epoch 65/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6022 - accuracy: 0.9299 - val_loss: 0.5928 - val_accuracy: 0.9409\n",
      "Epoch 66/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5890 - accuracy: 0.9283 - val_loss: 0.5766 - val_accuracy: 0.9458\n",
      "Epoch 67/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5753 - accuracy: 0.9305 - val_loss: 0.5610 - val_accuracy: 0.9458\n",
      "Epoch 68/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5621 - accuracy: 0.9365 - val_loss: 0.5480 - val_accuracy: 0.9458\n",
      "Epoch 69/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5493 - accuracy: 0.9360 - val_loss: 0.5401 - val_accuracy: 0.9458\n",
      "Epoch 70/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5373 - accuracy: 0.9403 - val_loss: 0.5263 - val_accuracy: 0.9458\n",
      "Epoch 71/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5249 - accuracy: 0.9409 - val_loss: 0.5120 - val_accuracy: 0.9458\n",
      "Epoch 72/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5133 - accuracy: 0.9447 - val_loss: 0.4999 - val_accuracy: 0.9507\n",
      "Epoch 73/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5014 - accuracy: 0.9502 - val_loss: 0.4928 - val_accuracy: 0.9458\n",
      "Epoch 74/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4905 - accuracy: 0.9502 - val_loss: 0.4817 - val_accuracy: 0.9458\n",
      "Epoch 75/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4801 - accuracy: 0.9546 - val_loss: 0.4668 - val_accuracy: 0.9507\n",
      "Epoch 76/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4691 - accuracy: 0.9562 - val_loss: 0.4616 - val_accuracy: 0.9507\n",
      "Epoch 77/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4583 - accuracy: 0.9628 - val_loss: 0.4453 - val_accuracy: 0.9606\n",
      "Epoch 78/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4484 - accuracy: 0.9589 - val_loss: 0.4392 - val_accuracy: 0.9606\n",
      "Epoch 79/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4389 - accuracy: 0.9650 - val_loss: 0.4293 - val_accuracy: 0.9606\n",
      "Epoch 80/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.9672 - val_loss: 0.4219 - val_accuracy: 0.9606\n",
      "Epoch 81/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4199 - accuracy: 0.9688 - val_loss: 0.4127 - val_accuracy: 0.9606\n",
      "Epoch 82/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4094 - accuracy: 0.9715 - val_loss: 0.3983 - val_accuracy: 0.9655\n",
      "Epoch 83/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4003 - accuracy: 0.9715 - val_loss: 0.3929 - val_accuracy: 0.9704\n",
      "Epoch 84/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3914 - accuracy: 0.9737 - val_loss: 0.3842 - val_accuracy: 0.9655\n",
      "Epoch 85/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.9748 - val_loss: 0.3709 - val_accuracy: 0.9704\n",
      "Epoch 86/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3742 - accuracy: 0.9748 - val_loss: 0.3673 - val_accuracy: 0.9704\n",
      "Epoch 87/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3659 - accuracy: 0.9765 - val_loss: 0.3588 - val_accuracy: 0.9754\n",
      "Epoch 88/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3572 - accuracy: 0.9787 - val_loss: 0.3526 - val_accuracy: 0.9754\n",
      "Epoch 89/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3494 - accuracy: 0.9808 - val_loss: 0.3398 - val_accuracy: 0.9754\n",
      "Epoch 90/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3413 - accuracy: 0.9814 - val_loss: 0.3341 - val_accuracy: 0.9754\n",
      "Epoch 91/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3334 - accuracy: 0.9830 - val_loss: 0.3250 - val_accuracy: 0.9754\n",
      "Epoch 92/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3259 - accuracy: 0.9836 - val_loss: 0.3197 - val_accuracy: 0.9803\n",
      "Epoch 93/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3187 - accuracy: 0.9830 - val_loss: 0.3121 - val_accuracy: 0.9803\n",
      "Epoch 94/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3112 - accuracy: 0.9819 - val_loss: 0.3040 - val_accuracy: 0.9803\n",
      "Epoch 95/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.9841 - val_loss: 0.3006 - val_accuracy: 0.9803\n",
      "Epoch 96/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2977 - accuracy: 0.9830 - val_loss: 0.2903 - val_accuracy: 0.9803\n",
      "Epoch 97/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2909 - accuracy: 0.9847 - val_loss: 0.2839 - val_accuracy: 0.9803\n",
      "Epoch 98/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.9852 - val_loss: 0.2783 - val_accuracy: 0.9803\n",
      "Epoch 99/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2781 - accuracy: 0.9858 - val_loss: 0.2725 - val_accuracy: 0.9803\n",
      "Epoch 100/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2720 - accuracy: 0.9869 - val_loss: 0.2665 - val_accuracy: 0.9803\n",
      "Epoch 101/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.9880 - val_loss: 0.2591 - val_accuracy: 0.9803\n",
      "Epoch 102/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.2592 - accuracy: 0.9858 - val_loss: 0.2595 - val_accuracy: 0.9803\n",
      "Epoch 103/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2532 - accuracy: 0.9885 - val_loss: 0.2474 - val_accuracy: 0.9803\n",
      "Epoch 104/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2479 - accuracy: 0.9880 - val_loss: 0.2433 - val_accuracy: 0.9803\n",
      "Epoch 105/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2423 - accuracy: 0.9880 - val_loss: 0.2372 - val_accuracy: 0.9803\n",
      "Epoch 106/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9896 - val_loss: 0.2316 - val_accuracy: 0.9803\n",
      "Epoch 107/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2313 - accuracy: 0.9891 - val_loss: 0.2293 - val_accuracy: 0.9852\n",
      "Epoch 108/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2262 - accuracy: 0.9901 - val_loss: 0.2239 - val_accuracy: 0.9852\n",
      "Epoch 109/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2210 - accuracy: 0.9912 - val_loss: 0.2191 - val_accuracy: 0.9803\n",
      "Epoch 110/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2161 - accuracy: 0.9912 - val_loss: 0.2142 - val_accuracy: 0.9852\n",
      "Epoch 111/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.9901 - val_loss: 0.2093 - val_accuracy: 0.9852\n",
      "Epoch 112/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9918 - val_loss: 0.2045 - val_accuracy: 0.9852\n",
      "Epoch 113/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9912 - val_loss: 0.1993 - val_accuracy: 0.9901\n",
      "Epoch 114/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9923 - val_loss: 0.1946 - val_accuracy: 0.9852\n",
      "Epoch 115/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9923 - val_loss: 0.1927 - val_accuracy: 0.9901\n",
      "Epoch 116/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9929 - val_loss: 0.1877 - val_accuracy: 0.9901\n",
      "Epoch 117/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9929 - val_loss: 0.1817 - val_accuracy: 0.9901\n",
      "Epoch 118/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.9945 - val_loss: 0.1772 - val_accuracy: 0.9901\n",
      "Epoch 119/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.9945 - val_loss: 0.1757 - val_accuracy: 0.9901\n",
      "Epoch 120/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9945 - val_loss: 0.1702 - val_accuracy: 0.9901\n",
      "Epoch 121/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9945 - val_loss: 0.1698 - val_accuracy: 0.9901\n",
      "Epoch 122/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9951 - val_loss: 0.1626 - val_accuracy: 0.9901\n",
      "Epoch 123/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9967 - val_loss: 0.1612 - val_accuracy: 0.9901\n",
      "Epoch 124/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9956 - val_loss: 0.1564 - val_accuracy: 0.9901\n",
      "Epoch 125/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9967 - val_loss: 0.1544 - val_accuracy: 0.9901\n",
      "Epoch 126/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9973 - val_loss: 0.1493 - val_accuracy: 0.9901\n",
      "Epoch 127/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9956 - val_loss: 0.1480 - val_accuracy: 0.9901\n",
      "Epoch 128/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9962 - val_loss: 0.1435 - val_accuracy: 0.9951\n",
      "Epoch 129/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9967 - val_loss: 0.1398 - val_accuracy: 0.9901\n",
      "Epoch 130/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9967 - val_loss: 0.1407 - val_accuracy: 0.9901\n",
      "Epoch 131/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9978 - val_loss: 0.1339 - val_accuracy: 0.9901\n",
      "Epoch 132/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9973 - val_loss: 0.1316 - val_accuracy: 0.9901\n",
      "Epoch 133/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9973 - val_loss: 0.1292 - val_accuracy: 0.9901\n",
      "Epoch 134/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9973 - val_loss: 0.1264 - val_accuracy: 0.9951\n",
      "Epoch 135/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9967 - val_loss: 0.1239 - val_accuracy: 0.9951\n",
      "Epoch 136/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9967 - val_loss: 0.1218 - val_accuracy: 0.9951\n",
      "Epoch 137/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9978 - val_loss: 0.1185 - val_accuracy: 0.9951\n",
      "Epoch 138/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9978 - val_loss: 0.1152 - val_accuracy: 0.9951\n",
      "Epoch 139/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9973 - val_loss: 0.1150 - val_accuracy: 0.9951\n",
      "Epoch 140/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9978 - val_loss: 0.1118 - val_accuracy: 0.9951\n",
      "Epoch 141/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9984 - val_loss: 0.1085 - val_accuracy: 0.9951\n",
      "Epoch 142/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9984 - val_loss: 0.1077 - val_accuracy: 0.9951\n",
      "Epoch 143/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9984 - val_loss: 0.1051 - val_accuracy: 0.9951\n",
      "Epoch 144/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9984 - val_loss: 0.1014 - val_accuracy: 0.9951\n",
      "Epoch 145/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9984 - val_loss: 0.1007 - val_accuracy: 0.9951\n",
      "Epoch 146/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9984 - val_loss: 0.0988 - val_accuracy: 0.9951\n",
      "Epoch 147/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9984 - val_loss: 0.0971 - val_accuracy: 0.9951\n",
      "Epoch 148/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9984 - val_loss: 0.0939 - val_accuracy: 0.9951\n",
      "Epoch 149/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9989 - val_loss: 0.0948 - val_accuracy: 0.9951\n",
      "Epoch 150/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9989 - val_loss: 0.0917 - val_accuracy: 0.9951\n",
      "Epoch 151/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9989 - val_loss: 0.0898 - val_accuracy: 0.9951\n",
      "Epoch 152/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9989 - val_loss: 0.0870 - val_accuracy: 0.9951\n",
      "Epoch 153/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9984 - val_loss: 0.0849 - val_accuracy: 0.9951\n",
      "Epoch 154/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0821 - accuracy: 0.9989 - val_loss: 0.0850 - val_accuracy: 0.9951\n",
      "Epoch 155/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9989 - val_loss: 0.0838 - val_accuracy: 0.9951\n",
      "Epoch 156/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9989 - val_loss: 0.0806 - val_accuracy: 0.9951\n",
      "Epoch 157/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9989 - val_loss: 0.0788 - val_accuracy: 0.9951\n",
      "Epoch 158/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9989 - val_loss: 0.0768 - val_accuracy: 0.9951\n",
      "Epoch 159/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9989 - val_loss: 0.0768 - val_accuracy: 0.9951\n",
      "Epoch 160/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9995 - val_loss: 0.0735 - val_accuracy: 0.9951\n",
      "Epoch 161/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9989 - val_loss: 0.0733 - val_accuracy: 0.9951\n",
      "Epoch 162/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9989 - val_loss: 0.0713 - val_accuracy: 0.9951\n",
      "Epoch 163/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9989 - val_loss: 0.0691 - val_accuracy: 0.9951\n",
      "Epoch 164/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9989 - val_loss: 0.0679 - val_accuracy: 0.9951\n",
      "Epoch 165/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9989 - val_loss: 0.0676 - val_accuracy: 0.9951\n",
      "Epoch 166/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.9989 - val_loss: 0.0666 - val_accuracy: 0.9951\n",
      "Epoch 167/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.9989 - val_loss: 0.0654 - val_accuracy: 0.9951\n",
      "Epoch 168/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.9989 - val_loss: 0.0636 - val_accuracy: 0.9951\n",
      "Epoch 169/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.9989 - val_loss: 0.0624 - val_accuracy: 0.9951\n",
      "Epoch 170/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0589 - accuracy: 0.9989 - val_loss: 0.0632 - val_accuracy: 0.9951\n",
      "Epoch 171/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9995 - val_loss: 0.0602 - val_accuracy: 1.0000\n",
      "Epoch 172/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9989 - val_loss: 0.0581 - val_accuracy: 1.0000\n",
      "Epoch 173/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0557 - accuracy: 0.9989 - val_loss: 0.0567 - val_accuracy: 1.0000\n",
      "Epoch 174/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0543 - accuracy: 0.9989 - val_loss: 0.0568 - val_accuracy: 0.9951\n",
      "Epoch 175/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9989 - val_loss: 0.0553 - val_accuracy: 1.0000\n",
      "Epoch 176/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9989 - val_loss: 0.0542 - val_accuracy: 0.9951\n",
      "Epoch 177/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0511 - accuracy: 0.9989 - val_loss: 0.0543 - val_accuracy: 0.9951\n",
      "Epoch 178/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0500 - accuracy: 0.9989 - val_loss: 0.0527 - val_accuracy: 0.9951\n",
      "Epoch 179/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9989 - val_loss: 0.0510 - val_accuracy: 0.9951\n",
      "Epoch 180/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0482 - accuracy: 0.9989 - val_loss: 0.0518 - val_accuracy: 0.9951\n",
      "Epoch 181/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0473 - accuracy: 0.9995 - val_loss: 0.0493 - val_accuracy: 0.9951\n",
      "Epoch 182/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0461 - accuracy: 0.9989 - val_loss: 0.0492 - val_accuracy: 0.9951\n",
      "Epoch 183/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0455 - accuracy: 0.9989 - val_loss: 0.0480 - val_accuracy: 1.0000\n",
      "Epoch 184/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0443 - accuracy: 0.9989 - val_loss: 0.0470 - val_accuracy: 0.9951\n",
      "Epoch 185/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0437 - accuracy: 0.9989 - val_loss: 0.0473 - val_accuracy: 1.0000\n",
      "Epoch 186/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9995 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
      "Epoch 187/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.0454 - val_accuracy: 1.0000\n",
      "Epoch 188/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0414 - accuracy: 0.9995 - val_loss: 0.0433 - val_accuracy: 1.0000\n",
      "Epoch 189/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0404 - accuracy: 0.9995 - val_loss: 0.0426 - val_accuracy: 1.0000\n",
      "Epoch 190/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0396 - accuracy: 0.9995 - val_loss: 0.0422 - val_accuracy: 0.9951\n",
      "Epoch 191/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 0.9951\n",
      "Epoch 192/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0383 - accuracy: 0.9989 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
      "Epoch 193/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
      "Epoch 194/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.9989 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
      "Epoch 195/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0359 - accuracy: 0.9995 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
      "Epoch 196/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9995 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
      "Epoch 197/2000\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.0345 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
      "Epoch 198/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0339 - accuracy: 0.9995 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 199/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0335 - accuracy: 0.9995 - val_loss: 0.0366 - val_accuracy: 1.0000\n",
      "Epoch 200/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 201/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
      "Epoch 202/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0315 - accuracy: 0.9995 - val_loss: 0.0344 - val_accuracy: 0.9951\n",
      "Epoch 203/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
      "Epoch 204/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
      "Epoch 205/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 0.9951\n",
      "Epoch 206/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
      "Epoch 207/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
      "Epoch 208/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 209/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 1.0000\n",
      "Epoch 210/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 211/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.0295 - val_accuracy: 1.0000\n",
      "Epoch 212/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 213/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
      "Epoch 214/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 215/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 216/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 217/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
      "Epoch 218/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 219/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
      "Epoch 220/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 221/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000\n",
      "Epoch 222/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0220 - accuracy: 0.9995 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
      "Epoch 223/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 1.0000\n",
      "Epoch 224/2000\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
      "Epoch 225/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 1.0000\n",
      "Epoch 226/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
      "Epoch 227/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "Epoch 228/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
      "Epoch 229/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
      "Epoch 230/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
      "Epoch 231/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
      "Epoch 232/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
      "Epoch 233/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
      "Epoch 234/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0203 - val_accuracy: 1.0000\n",
      "Epoch 235/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 1.0000\n",
      "Epoch 236/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
      "Epoch 237/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
      "Epoch 238/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
      "Epoch 239/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
      "Epoch 240/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
      "Epoch 241/2000\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
      "Epoch 242/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
      "Epoch 243/2000\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
      "Epoch 244/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
      "Epoch 245/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 1.0000\n",
      "Epoch 246/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
      "Epoch 247/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
      "Epoch 248/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
      "Epoch 249/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
      "Epoch 250/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 251/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
      "Epoch 252/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000\n",
      "Epoch 253/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
      "Epoch 254/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 255/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000\n",
      "Epoch 256/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 257/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
      "Epoch 258/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 259/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
      "Epoch 260/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
      "Epoch 261/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 262/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
      "Epoch 263/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 264/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 265/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 266/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 267/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
      "Epoch 268/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
      "Epoch 269/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 270/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
      "Epoch 271/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 272/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
      "Epoch 273/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 274/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 275/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
      "Epoch 276/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
      "Epoch 277/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 278/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 279/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 280/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 281/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 282/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
      "Epoch 283/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "Epoch 284/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "Epoch 285/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 286/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "Epoch 287/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 288/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 289/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 290/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
      "Epoch 291/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
      "Epoch 292/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 293/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 294/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 295/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 296/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 297/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 298/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 299/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 300/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 301/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
      "Epoch 302/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 303/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0076 - val_accuracy: 1.0000\n",
      "Epoch 304/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 305/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 306/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 307/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 308/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
      "Epoch 309/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
      "Epoch 310/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 311/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 312/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 313/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 314/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 315/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
      "Epoch 316/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 317/2000\n",
      "68/68 [==============================] - 0s 881us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "Epoch 318/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 319/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
      "Epoch 320/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 321/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0058 - val_accuracy: 1.0000\n",
      "Epoch 322/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 323/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 324/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 325/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 326/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 327/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 328/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 329/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 330/2000\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 331/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 332/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 333/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 334/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 335/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 336/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 337/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 338/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 339/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 340/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 341/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 342/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 343/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 344/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 345/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 346/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 347/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 348/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 349/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 350/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 351/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 352/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 353/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 354/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 355/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 356/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 357/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 358/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 359/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 360/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 361/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 362/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 363/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 364/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 365/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 366/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 367/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 368/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 369/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 370/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 371/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 372/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 373/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 374/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 375/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 376/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 377/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 378/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 379/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 380/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 381/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 382/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 383/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 384/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 385/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 386/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 387/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 388/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 389/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 390/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 391/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 392/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 393/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 394/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 395/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 396/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 397/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 398/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 399/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 400/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 401/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 402/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 403/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 404/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 405/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 406/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 407/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 408/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 409/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 410/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 411/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 412/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 413/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 414/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 415/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 416/2000\n",
      "68/68 [==============================] - 0s 940us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 417/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 418/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 419/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 420/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 421/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 422/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 423/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 424/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 425/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 426/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 427/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 428/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 429/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 430/2000\n",
      "68/68 [==============================] - 0s 940us/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 431/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 9.6670e-04 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 432/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 9.7937e-04 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 433/2000\n",
      "68/68 [==============================] - 0s 940us/step - loss: 9.4765e-04 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 434/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 9.4442e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 435/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 9.6144e-04 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 436/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 9.4935e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 437/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 9.0582e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 438/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 9.1670e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 439/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 8.9734e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 440/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 8.6936e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 441/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 8.8841e-04 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 442/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 443/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 9.1907e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 444/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 8.4149e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 445/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 8.1154e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 446/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 8.3581e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 447/2000\n",
      "68/68 [==============================] - 0s 970us/step - loss: 8.0805e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 448/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 7.8943e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 449/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 7.9023e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 450/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 7.7054e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 451/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 7.8755e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 452/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 7.5682e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 453/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 7.4378e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 454/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 7.3040e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 455/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 7.3837e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 456/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 7.2726e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 457/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 7.3545e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 458/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 7.1073e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 459/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 7.0422e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 460/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 7.1290e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 461/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 7.1208e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 462/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 6.7574e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 463/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 6.7685e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 464/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 6.5883e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 465/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 6.5759e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 466/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 6.5381e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 467/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 6.9061e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 468/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 6.5079e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 469/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 6.2146e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 470/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 6.2856e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 471/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 6.0936e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 472/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 6.2903e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 473/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 6.5110e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 474/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 5.9870e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 475/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.8155e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 476/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 5.6992e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 477/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 5.6893e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 478/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.7966e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 479/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 5.5094e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 480/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.6807e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 481/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 5.4329e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 482/2000\n",
      "68/68 [==============================] - 0s 881us/step - loss: 5.3587e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 483/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.2239e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 484/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 5.2276e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 485/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.1675e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 486/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.0695e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 487/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 5.2345e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 488/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.0976e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 489/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.0176e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 490/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.1343e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 491/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 5.6667e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 492/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.1755e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 493/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 4.6991e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 494/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 4.8767e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 495/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 4.8129e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 496/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 4.4539e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 497/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.4063e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 498/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 5.1836e-04 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 499/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 9.1834e-04 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 500/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 4.4306e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 501/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 4.1994e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 502/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 4.1668e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 503/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 4.1502e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 504/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 4.1440e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 505/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 4.0537e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 506/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 4.0673e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 507/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.9978e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 508/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 3.9259e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 509/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 3.9354e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 510/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.9002e-04 - accuracy: 1.0000 - val_loss: 9.3064e-04 - val_accuracy: 1.0000\n",
      "Epoch 511/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 3.8345e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 512/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.8755e-04 - accuracy: 1.0000 - val_loss: 9.8373e-04 - val_accuracy: 1.0000\n",
      "Epoch 513/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.7994e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 514/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.7752e-04 - accuracy: 1.0000 - val_loss: 9.6824e-04 - val_accuracy: 1.0000\n",
      "Epoch 515/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 3.7796e-04 - accuracy: 1.0000 - val_loss: 9.3729e-04 - val_accuracy: 1.0000\n",
      "Epoch 516/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 3.6506e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 517/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.6287e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 518/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 3.6068e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 519/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 3.6000e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 520/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.5089e-04 - accuracy: 1.0000 - val_loss: 9.8645e-04 - val_accuracy: 1.0000\n",
      "Epoch 521/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 3.4646e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 522/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.4178e-04 - accuracy: 1.0000 - val_loss: 8.6196e-04 - val_accuracy: 1.0000\n",
      "Epoch 523/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 3.4888e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 524/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.4414e-04 - accuracy: 1.0000 - val_loss: 9.7518e-04 - val_accuracy: 1.0000\n",
      "Epoch 525/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.3974e-04 - accuracy: 1.0000 - val_loss: 8.5046e-04 - val_accuracy: 1.0000\n",
      "Epoch 526/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 3.2997e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 527/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.2622e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 528/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 3.2178e-04 - accuracy: 1.0000 - val_loss: 9.9000e-04 - val_accuracy: 1.0000\n",
      "Epoch 529/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.2009e-04 - accuracy: 1.0000 - val_loss: 9.3068e-04 - val_accuracy: 1.0000\n",
      "Epoch 530/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 3.2466e-04 - accuracy: 1.0000 - val_loss: 9.4425e-04 - val_accuracy: 1.0000\n",
      "Epoch 531/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 3.0964e-04 - accuracy: 1.0000 - val_loss: 9.4828e-04 - val_accuracy: 1.0000\n",
      "Epoch 532/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.1007e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 533/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 3.0979e-04 - accuracy: 1.0000 - val_loss: 9.3296e-04 - val_accuracy: 1.0000\n",
      "Epoch 534/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.0297e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 535/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 3.1939e-04 - accuracy: 1.0000 - val_loss: 8.6362e-04 - val_accuracy: 1.0000\n",
      "Epoch 536/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 3.0483e-04 - accuracy: 1.0000 - val_loss: 9.1723e-04 - val_accuracy: 1.0000\n",
      "Epoch 537/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.0030e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 538/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 7.7547e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 539/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 540/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 5.9352e-04 - accuracy: 1.0000 - val_loss: 6.8116e-04 - val_accuracy: 1.0000\n",
      "Epoch 541/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.9238e-04 - accuracy: 1.0000 - val_loss: 7.5693e-04 - val_accuracy: 1.0000\n",
      "Epoch 542/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.8523e-04 - accuracy: 1.0000 - val_loss: 8.0446e-04 - val_accuracy: 1.0000\n",
      "Epoch 543/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 2.8017e-04 - accuracy: 1.0000 - val_loss: 8.7244e-04 - val_accuracy: 1.0000\n",
      "Epoch 544/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.7601e-04 - accuracy: 1.0000 - val_loss: 8.9574e-04 - val_accuracy: 1.0000\n",
      "Epoch 545/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 2.7362e-04 - accuracy: 1.0000 - val_loss: 9.2204e-04 - val_accuracy: 1.0000\n",
      "Epoch 546/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.7163e-04 - accuracy: 1.0000 - val_loss: 9.3962e-04 - val_accuracy: 1.0000\n",
      "Epoch 547/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 2.6981e-04 - accuracy: 1.0000 - val_loss: 9.5986e-04 - val_accuracy: 1.0000\n",
      "Epoch 548/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.6825e-04 - accuracy: 1.0000 - val_loss: 9.4785e-04 - val_accuracy: 1.0000\n",
      "Epoch 549/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.6678e-04 - accuracy: 1.0000 - val_loss: 9.7738e-04 - val_accuracy: 1.0000\n",
      "Epoch 550/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.6443e-04 - accuracy: 1.0000 - val_loss: 9.4541e-04 - val_accuracy: 1.0000\n",
      "Epoch 551/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.6193e-04 - accuracy: 1.0000 - val_loss: 9.2357e-04 - val_accuracy: 1.0000\n",
      "Epoch 552/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 2.6059e-04 - accuracy: 1.0000 - val_loss: 9.1900e-04 - val_accuracy: 1.0000\n",
      "Epoch 553/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 2.5947e-04 - accuracy: 1.0000 - val_loss: 9.1911e-04 - val_accuracy: 1.0000\n",
      "Epoch 554/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.5737e-04 - accuracy: 1.0000 - val_loss: 9.4872e-04 - val_accuracy: 1.0000\n",
      "Epoch 555/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.5879e-04 - accuracy: 1.0000 - val_loss: 8.6758e-04 - val_accuracy: 1.0000\n",
      "Epoch 556/2000\n",
      "68/68 [==============================] - 0s 881us/step - loss: 2.5372e-04 - accuracy: 1.0000 - val_loss: 8.9325e-04 - val_accuracy: 1.0000\n",
      "Epoch 557/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.5177e-04 - accuracy: 1.0000 - val_loss: 8.8352e-04 - val_accuracy: 1.0000\n",
      "Epoch 558/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 2.5151e-04 - accuracy: 1.0000 - val_loss: 8.6893e-04 - val_accuracy: 1.0000\n",
      "Epoch 559/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.4928e-04 - accuracy: 1.0000 - val_loss: 9.3620e-04 - val_accuracy: 1.0000\n",
      "Epoch 560/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 2.4703e-04 - accuracy: 1.0000 - val_loss: 8.9690e-04 - val_accuracy: 1.0000\n",
      "Epoch 561/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.4494e-04 - accuracy: 1.0000 - val_loss: 8.3861e-04 - val_accuracy: 1.0000\n",
      "Epoch 562/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.4283e-04 - accuracy: 1.0000 - val_loss: 8.4776e-04 - val_accuracy: 1.0000\n",
      "Epoch 563/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 2.4157e-04 - accuracy: 1.0000 - val_loss: 8.6670e-04 - val_accuracy: 1.0000\n",
      "Epoch 564/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.4035e-04 - accuracy: 1.0000 - val_loss: 8.2535e-04 - val_accuracy: 1.0000\n",
      "Epoch 565/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 2.3797e-04 - accuracy: 1.0000 - val_loss: 8.4242e-04 - val_accuracy: 1.0000\n",
      "Epoch 566/2000\n",
      "68/68 [==============================] - 0s 881us/step - loss: 2.3815e-04 - accuracy: 1.0000 - val_loss: 8.1756e-04 - val_accuracy: 1.0000\n",
      "Epoch 567/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.3674e-04 - accuracy: 1.0000 - val_loss: 8.5404e-04 - val_accuracy: 1.0000\n",
      "Epoch 568/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 2.3543e-04 - accuracy: 1.0000 - val_loss: 8.2721e-04 - val_accuracy: 1.0000\n",
      "Epoch 569/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.3209e-04 - accuracy: 1.0000 - val_loss: 9.3246e-04 - val_accuracy: 1.0000\n",
      "Epoch 570/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.3133e-04 - accuracy: 1.0000 - val_loss: 7.9419e-04 - val_accuracy: 1.0000\n",
      "Epoch 571/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.2864e-04 - accuracy: 1.0000 - val_loss: 8.2279e-04 - val_accuracy: 1.0000\n",
      "Epoch 572/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.2785e-04 - accuracy: 1.0000 - val_loss: 8.3227e-04 - val_accuracy: 1.0000\n",
      "Epoch 573/2000\n",
      "68/68 [==============================] - 0s 941us/step - loss: 2.2490e-04 - accuracy: 1.0000 - val_loss: 8.0931e-04 - val_accuracy: 1.0000\n",
      "Epoch 574/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.2548e-04 - accuracy: 1.0000 - val_loss: 8.5135e-04 - val_accuracy: 1.0000\n",
      "Epoch 575/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.2124e-04 - accuracy: 1.0000 - val_loss: 8.2387e-04 - val_accuracy: 1.0000\n",
      "Epoch 576/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.1810e-04 - accuracy: 1.0000 - val_loss: 8.0732e-04 - val_accuracy: 1.0000\n",
      "Epoch 577/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.1809e-04 - accuracy: 1.0000 - val_loss: 7.7862e-04 - val_accuracy: 1.0000\n",
      "Epoch 578/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.1604e-04 - accuracy: 1.0000 - val_loss: 8.2902e-04 - val_accuracy: 1.0000\n",
      "Epoch 579/2000\n",
      "68/68 [==============================] - 0s 985us/step - loss: 2.1400e-04 - accuracy: 1.0000 - val_loss: 8.3551e-04 - val_accuracy: 1.0000\n",
      "Epoch 580/2000\n",
      "68/68 [==============================] - 0s 911us/step - loss: 2.1209e-04 - accuracy: 1.0000 - val_loss: 7.3821e-04 - val_accuracy: 1.0000\n",
      "Epoch 581/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 2.1453e-04 - accuracy: 1.0000 - val_loss: 7.3848e-04 - val_accuracy: 1.0000\n",
      "Epoch 582/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.1186e-04 - accuracy: 1.0000 - val_loss: 8.5319e-04 - val_accuracy: 1.0000\n",
      "Epoch 583/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 2.0740e-04 - accuracy: 1.0000 - val_loss: 7.3939e-04 - val_accuracy: 1.0000\n",
      "Epoch 584/2000\n",
      "68/68 [==============================] - 0s 896us/step - loss: 2.0865e-04 - accuracy: 1.0000 - val_loss: 7.9660e-04 - val_accuracy: 1.0000\n",
      "Epoch 585/2000\n",
      "68/68 [==============================] - 0s 955us/step - loss: 2.0644e-04 - accuracy: 1.0000 - val_loss: 7.4311e-04 - val_accuracy: 1.0000\n",
      "Epoch 586/2000\n",
      "68/68 [==============================] - 0s 970us/step - loss: 2.0358e-04 - accuracy: 1.0000 - val_loss: 7.4464e-04 - val_accuracy: 1.0000\n",
      "Epoch 587/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.0397e-04 - accuracy: 1.0000 - val_loss: 7.0978e-04 - val_accuracy: 1.0000\n",
      "Epoch 588/2000\n",
      "68/68 [==============================] - 0s 926us/step - loss: 2.0073e-04 - accuracy: 1.0000 - val_loss: 7.2595e-04 - val_accuracy: 1.0000\n",
      "Epoch 589/2000\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.9682e-04 - accuracy: 1.0000 - val_loss: 7.1754e-04 - val_accuracy: 1.0000\n",
      "Epoch 590/2000\n",
      "68/68 [==============================] - 0s 970us/step - loss: 1.9850e-04 - accuracy: 1.0000 - val_loss: 7.3247e-04 - val_accuracy: 1.0000\n",
      "Epoch 00590: early stopping\n",
      "12/12 - 0s - loss: 0.0127 - accuracy: 0.9947 - 361ms/epoch - 30ms/step\n"
     ]
    }
   ],
   "source": [
    "# Path to train\n",
    "path_to_train=root_folder_+\"C/2500/Z_012_TRAIN_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Path to test\n",
    "path_to_test=root_folder_+\"3/2500/Z_3_TEST_70_STEP_2500_OVERLAP_50_PERCERNT_LPC_50_.csv\"\n",
    "\n",
    "# Define an array to save reults of each trial of each expirement\n",
    "# It is used to calculate the average of each expirement\n",
    "temp_results=np.zeros(1)\n",
    "\n",
    "#Loading training data\n",
    "data = pd.read_csv(path_to_train,header=None)\n",
    "data=data.sort_values(by=[51],  ascending=True)\n",
    "\n",
    "# data = data.iloc[450:]\n",
    "\n",
    "g = data.groupby([51])\n",
    "data=g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "\n",
    "\n",
    "labels=data.iloc[:,51]\n",
    "features = data.iloc[:,1:51]\n",
    "X=features\n",
    "y=np.ravel(labels)\n",
    "\n",
    "print(data)\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "     X_train, y_train = shuffle(X, y)\n",
    "     print(X_train.shape)\n",
    "\n",
    "     #Loading testing data\n",
    "     data12 = pd.read_csv(path_to_test,header=None)\n",
    "     labels12=data12.iloc[:,51]\n",
    "     features12 = data12.iloc[:,1:51]\n",
    "     X12=features12\n",
    "     y12=np.ravel(labels12)\n",
    "     X_test=X12;\n",
    "     y_test=y12;\n",
    "     print(X_test.shape)\n",
    "\n",
    "\n",
    "     # Making target data in range [0,9]\n",
    "     y_train=y_train-1;\n",
    "     y_test=y_test-1;\n",
    "\n",
    "     #Convert to categorical\n",
    "     Y_train = np_utils.to_categorical(y_train, myN) \n",
    "     Y_test = np_utils.to_categorical(y_test, myN)\n",
    "     \n",
    "\n",
    "     # Training model\n",
    "     checkpoint_path=root_folder_Models+'/bestmodel_012_3.h5'\n",
    "     keras_callbacks   = [\n",
    "           EarlyStopping(monitor='val_loss', patience=50, verbose=1),\n",
    "           ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "     ]\n",
    "\n",
    "\n",
    "     model=build_my_model(myN);\n",
    "     history=model.fit(X_train, Y_train,epochs=2000, batch_size=27, verbose=1, validation_split=0.1,callbacks=[keras_callbacks])\n",
    "     best_model = load_model(checkpoint_path)\n",
    "     model=best_model;\n",
    "     #Calculate score\n",
    "     scores = model.evaluate(X_test, Y_test, verbose=2) \n",
    "     temp_results[i]=scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy value while using testing dataset: 99.47%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy value while using testing dataset: %.2f%%\" % (np.mean(temp_results)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
